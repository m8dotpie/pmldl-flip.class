@misc{chamberlainNeuralEmbeddingsGraphs2017,
  title = {Neural {{Embeddings}} of {{Graphs}} in {{Hyperbolic Space}}},
  author = {Chamberlain, Benjamin Paul and Clough, James and Deisenroth, Marc Peter},
  year = {2017},
  month = may,
  number = {arXiv:1705.10359},
  eprint = {1705.10359},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Neural embeddings have been used with great success in Natural Language Processing (NLP). They provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph based tasks, embeddings have been learned in high-dimensional Euclidean spaces. However, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but negatively curved, hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that embedding graphs in their natural geometry significantly improves performance on downstream tasks for several real-world public datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/m8dotpie/Zotero/storage/9LCSU2LX/Chamberlain et al. - 2017 - Neural Embeddings of Graphs in Hyperbolic Space.pdf;/Users/m8dotpie/Zotero/storage/NX7LJCRE/1705.html}
}

@misc{chenNeuralOrdinaryDifferential2019,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  year = {2019},
  month = dec,
  number = {arXiv:1806.07366},
  eprint = {1806.07366},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/m8dotpie/Zotero/storage/WVLWZZAM/Chen et al. - 2019 - Neural Ordinary Differential Equations.pdf;/Users/m8dotpie/Zotero/storage/XXADJKD6/1806.html}
}

@misc{davidsonHypersphericalVariationalAutoEncoders2022,
  title = {Hyperspherical {{Variational Auto-Encoders}}},
  author = {Davidson, Tim R. and Falorsi, Luca and De Cao, Nicola and Kipf, Thomas and Tomczak, Jakub M.},
  year = {2022},
  month = sep,
  number = {arXiv:1804.00891},
  eprint = {1804.00891},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {The Variational Auto-Encoder (VAE) is one of the most used unsupervised machine learning models. But although the default choice of a Gaussian distribution for both the prior and posterior represents a mathematically convenient distribution often leading to competitive results, we show that this parameterization fails to model data with a latent hyperspherical structure. To address this issue we propose using a von Mises-Fisher (vMF) distribution instead, leading to a hyperspherical latent space. Through a series of experiments we show how such a hyperspherical VAE, or \$\textbackslash mathcal\{S\}\$-VAE, is more suitable for capturing data with a hyperspherical latent structure, while outperforming a normal, \$\textbackslash mathcal\{N\}\$-VAE, in low dimensions on other data types. Code at http://github.com/nicola-decao/s-vae-tf and https://github.com/nicola-decao/s-vae-pytorch},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/m8dotpie/Zotero/storage/4EEAHLG6/Davidson et al. - 2022 - Hyperspherical Variational Auto-Encoders.pdf;/Users/m8dotpie/Zotero/storage/J5B6VZI7/1804.html}
}

@misc{kidgerNeuralDifferentialEquations2022,
  title = {On {{Neural Differential Equations}}},
  author = {Kidger, Patrick},
  year = {2022},
  month = feb,
  number = {arXiv:2202.02435},
  eprint = {2202.02435},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  abstract = {The conjoining of dynamical systems and deep learning has become a topic of great interest. In particular, neural differential equations (NDEs) demonstrate that neural networks and differential equation are two sides of the same coin. Traditional parameterised differential equations are a special case. Many popular neural network architectures, such as residual networks and recurrent networks, are discretisations. NDEs are suitable for tackling generative problems, dynamical systems, and time series (particularly in physics, finance, ...) and are thus of interest to both modern machine learning and traditional mathematical modelling. NDEs offer high-capacity function approximation, strong priors on model space, the ability to handle irregular data, memory efficiency, and a wealth of available theory on both sides. This doctoral thesis provides an in-depth survey of the field. Topics include: neural ordinary differential equations (e.g. for hybrid neural/mechanistic modelling of physical systems); neural controlled differential equations (e.g. for learning functions of irregular time series); and neural stochastic differential equations (e.g. to produce generative models capable of representing complex stochastic dynamics, or sampling from complex high-dimensional distributions). Further topics include: numerical methods for NDEs (e.g. reversible differential equations solvers, backpropagation through differential equations, Brownian reconstruction); symbolic regression for dynamical systems (e.g. via regularised evolution); and deep implicit models (e.g. deep equilibrium models, differentiable optimisation). We anticipate this thesis will be of interest to anyone interested in the marriage of deep learning with dynamical systems, and hope it will provide a useful reference for the current state of the art.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Classical Analysis and ODEs,Mathematics - Dynamical Systems,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/Users/m8dotpie/Zotero/storage/8STUF8G5/Kidger - 2022 - On Neural Differential Equations.pdf;/Users/m8dotpie/Zotero/storage/SPU92EFC/2202.html}
}

@misc{saemundssonVariationalIntegratorNetworks2020,
  title = {Variational {{Integrator Networks}} for {{Physically Structured Embeddings}}},
  author = {Saemundsson, Steindor and Terenin, Alexander and Hofmann, Katja and Deisenroth, Marc Peter},
  year = {2020},
  month = mar,
  number = {arXiv:1910.09349},
  eprint = {1910.09349},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Learning workable representations of dynamical systems is becoming an increasingly important problem in a number of application areas. By leveraging recent work connecting deep neural networks to systems of differential equations, we propose \textbackslash emph\{variational integrator networks\}, a class of neural network architectures designed to preserve the geometric structure of physical systems. This class of network architectures facilitates accurate long-term prediction, interpretability, and data-efficient learning, while still remaining highly flexible and capable of modeling complex behavior. We demonstrate that they can accurately learn dynamical systems from both noisy observations in phase space and from image pixels within which the unknown dynamics are embedded.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/m8dotpie/Zotero/storage/MPNI8AN4/Saemundsson et al. - 2020 - Variational Integrator Networks for Physically Str.pdf;/Users/m8dotpie/Zotero/storage/H26U7Z8Q/1910.html}
}

@misc{xiongGeometricRelationalEmbeddings2023,
  title = {Geometric {{Relational Embeddings}}: {{A Survey}}},
  shorttitle = {Geometric {{Relational Embeddings}}},
  author = {Xiong, Bo and Nayyeri, Mojtaba and Jin, Ming and He, Yunjie and Cochez, Michael and Pan, Shirui and Staab, Steffen},
  year = {2023},
  month = apr,
  number = {arXiv:2304.11949},
  eprint = {2304.11949},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Geometric relational embeddings map relational data as geometric objects that combine vector information suitable for machine learning and structured/relational information for structured/relational reasoning, typically in low dimensions. Their preservation of relational structures and their appealing properties and interpretability have led to their uptake for tasks such as knowledge graph completion, ontology and hierarchy reasoning, logical query answering, and hierarchical multi-label classification. We survey methods that underly geometric relational embeddings and categorize them based on (i) the embedding geometries that are used to represent the data; and (ii) the relational reasoning tasks that they aim to improve. We identify the desired properties (i.e., inductive biases) of each kind of embedding and discuss some potential future work.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/m8dotpie/Zotero/storage/89UVRSED/Xiong et al. - 2023 - Geometric Relational Embeddings A Survey.pdf;/Users/m8dotpie/Zotero/storage/J2KAYPVH/2304.html}
}

@misc{yinMathematicalUnderstandingResNet2019,
  title = {On the {{Mathematical Understanding}} of {{ResNet}} with {{Feynman Path Integral}}},
  author = {Yin, Minghao and Li, Xiu and Zhang, Yongbing and Wang, Shiqi},
  year = {2019},
  month = apr,
  number = {arXiv:1904.07568},
  eprint = {1904.07568},
  primaryclass = {hep-th, stat},
  publisher = {{arXiv}},
  abstract = {In this paper, we aim to understand Residual Network (ResNet) in a scientifically sound way by providing a bridge between ResNet and Feynman path integral. In particular, we prove that the effect of residual block is equivalent to partial differential equation, and the ResNet transforming process can be equivalently converted to Feynman path integral. These conclusions greatly help us mathematically understand the advantage of ResNet in addressing the gradient vanishing issue. More importantly, our analyses offer a path integral view of ResNet, and demonstrate that the output of certain network can be obtained by adding contributions of all paths. Moreover, the contribution of each path is proportional to e\^\{-S\}, where S is the action given by time integral of Lagrangian L. This lays the solid foundation in the understanding of ResNet, and provides insights in the future design of convolutional neural network architecture. Based on these results, we have designed the network using partial differential operators, which further validates our theoritical analyses.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,High Energy Physics - Theory,Statistics - Machine Learning},
  file = {/Users/m8dotpie/Zotero/storage/EMH69P3M/Yin et al. - 2019 - On the Mathematical Understanding of ResNet with F.pdf;/Users/m8dotpie/Zotero/storage/DC7WLLIX/1904.html}
}
